---
title: "Serial dependence in random binary sequences"
date: 2020-04-05
draft: true
katex: true
markup: "mmark"
modified: 2020-04-06
---

In this blog post I am going to investigate serial dependence in random binary
sequences.

* Binary sequences are sequences of of zeros and ones. 

It's about binary sequences that flag occurrence of some random events. In this
context, we observe a system at discrete time instances and produce zeros if
not event occurred and one otherwise.  

TODO: Describe a simple example and work with in through the article.

## Remarks to be mixed into the text later

* I am interested in the statistical formulation of the problem. Given a finite
  sequence of zeros and ones, collect a set of methods to gather arguments to
  support or reject the hypothesis that this sequence was generated by sampling
  from a Bernoulli random variable with success probability $$p$$.
* Is a given sequence a random noise or is there some underlying pattern. 
* In particular, I am interested in testing whether there is a dependence
  between the events. Do these events tend to occur in clusters? 

## Probabilistic formulation of the problem.

Given a sequence or random variables $$X = (X_0, X_1, ..., X_n)$$ taking values
in the set $${0, 1}$$ and a probability $$p\in(0,1)$$. Moreover, I have a
sample drawn from $$X$$, i.e. a sequence of zeros and ones 
$$x_0, x_1, ..., x_n$$. I would like to investigate whether 

1. $$X_i$$ have all the same distribution $$\text{Ber}(p)$$ (check if the
   occurrence probability is really $$p$$), and
2. whether $$X_i$$ are independent (check if there is a dependence between the
   events). 

Simpler question: Was the sequence $$x_0, x_1, ..., x_n$$ generated by
independent sampling from a Bernoulli random variable $$X\sim\text{Ber}(p)$$.

This is a surprisingly difficult and deep problem.

If we assume that the events are i.i.d. samples from a fixed Bernoulli random
variable $$X_0$$, then we can estimate the probability $$p$$ by calculating the
average of $$x_0, x_1, ..., x_n$$.  This is because the expectation of $$X_0$$
is $$\mathbb E X_0 = p$$.

## Serial dependence

How can we check whether our events occur independently of each other and 
gather evidence that there is not serial dependence? 

Examples of dependent events is when occurrence of an event some how impacts
the occurrence probabilities of subsequent events. Typical example is when
something breaks and gets repaired. 

Lucky and unlucky streaks. Run of luck. Winning streak. 

Mention the problem of run probabilities and the our intuitive understanding of
independence is often not correct.

In order to investigate the serial dependence in our sequence we can look at
the distribution of the lags (or waiting times between the events). For example
the sequence 

    [0, 1, 1, 0, 0, 1, 0, 0],

yields the following sequence of waiting times

    [0, 2].

Note that the waiting time calculation discards the initial and trailing zeros
of the event sequence. For an i.i.d. sequence of Bernoulli random variables, the
sequene of waiting times consists of i.i.d. rv with geometric distribution
with parameter $$p$$. The probability mass function of the geometric
distribution is given by 

$$w(k) = \mathbb P(Z = k) = (1-p)^{k-1} p,\ k=1,2,...$$

and describes the waiting time probabilities. The probability mass distribution
(PMF) for $$p=0.1$$ is given by

{{< figure src="/binary_sequences/geom_pmf.png" >}}

One way of test whether a waiting time distribution follows geometric
distribution is to look at the orthogonal polynomials generated by
that distribution.

Orthogonal polynomials are an old technology.

For a given probability distribution $$\mu$$ on $$\mathbb R$$ we can 
define a scalar product between square integrable functions $$f, g \in L_2(\mu)$$
as 

$$\langle f,g \rangle = \int f(x) g(x) \mu(dx) = \mathbb E f(Y)g(Y),$$

where $$Y$$ is a random variable with distribution $$\mu$$.
Also a scalar product defines the notion of orthogonality between the functions
in $$L_2(\mu)$$: $$f$$ and $$g$$ are orthogonal iff $$\langle f,g \rangle = 0$$.
Finally, as sequence of polynomials $$(p_i(x))_{i\geq 0}$$ is called orthogonal 
if $$\langle p_k, p_i \rangle = 0$$ for all $$k \neq i$$. 

In this light, there is an intimate connection between probability distributions
and families of orthogonal polynomials.

For a geometric probability distribution characterized by the above pmf, where
is a corresponding family of orthogonal polynomials $$M = (M_i(x))_{i\geq 0}$$.
The family $$M$$ is a special case of the Meixner family of orthogonal
polynomials that is derived from negative binomail distribution (a
generalization of the geometric distribution). The members of the Meixner
family satisfy the following handy recursive relation:

$$ M_{j+1}(x) = \frac{ (1-p)(2j+1) + p(j-x+1)}{(j+1)\sqrt{1-p}} M_j(x) - \frac{j}{j+1} M_{j-1}(x), $$

with $$M_1(x) = 1$$ and $$M_{-1}(x) = 0$$. This relation is used to calculate 
the sequence $$M$$.

{{< figure src="/binary_sequences/meixner_polynomials.png" >}}

As mentioned above, every polynomial $$M_k(x)$$ in $$M$$ satisfies the equation
$$\mathbb E M_k(Y) = 0$$ for a geometric random variable $$Y$$. This relation can
be used to test whether a given sample of waiting times belongs to a geometric
distribution.  We are going to calculate the empirical expectation and use that
as a score: Values close to zero can be interpreted as evidence speaking for
geometric distribution.  If the values are far from zero, this is a sign that
we need to revisit our assumptions and possibly discard the i.i.d. hypothesis
about the original event flag sequence.


## Examples

* VaR violations. 
* Random number generators usually produce binary sequences that are
  transformed deterministically into some "more interesting" random numbers.
  There are test suites that test how random is a sequence produced by a rng.
  Example: Die hard test suite.
* Kaggle competitions or data sets?

## Use cases

* Value at Risk testing. 

## Ideas

* Train a Markov process and look at it's transition probability matrix.


## References

* Bertrand Candelon, Gilbert Colletaz, Christophe Hurlin, Sessi Tokpavi,
  Backtesting Value-at-Risk: A GMM Duration-Based Test, Journal of Financial
  Econometrics, Volume 9, Issue 2, Spring 2011, Pages 314â€“343,
  http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.404.3188&rep=rep1&type=pdf
* Christoffersen, Peter and Pelletier, Denis, Backtesting Value-at-Risk: A
  Duration-Based Approach (January 31, 2003). Available at SSRN:
  https://ssrn.com/abstract=418762 or http://dx.doi.org/10.2139/ssrn.418762 
* Olver, Frank WJ, Daniel W. Lozier, Ronald F. Boisvert, and Charles W. Clark,
  eds. NIST handbook of mathematical functions hardback and CD-ROM. Cambridge
  university press, 2010.


<!-- vim: set syntax=markdown: set spelllang=en: set spell: -->
