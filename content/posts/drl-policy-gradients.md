---
title: "The Reacher enviroment solved using policy gradients"
date: 2019-04-15
lastmod: 2019-04-15
draft: true
katex: true
markup: "mmark"
tags:
    - reinforcement-learning
    - machine-learning
---


# Abstract 
This short note provides a concise description of the model architecture and
learning algorithms of the agent developed in this project. We also report learning
performance of the agent and provide a list of possible future model improvements.

# Description of the learning algorithm

Requirement: The report clearly describes the learning algorithm, along with the chosen hyperparameters. It also describes the model architectures for any neural networks.

The algorithm used to solve the problem posed in this project closely resembles
the algorithm proposed in \cite{mnih2015humanlevel}.

The agent is trained over a given number of episodes labeled with $$n = 1,\cdots, 1800$$.
Each episode is divided into turns $$t = 1,\cdots, 300$$.
The state space $\mathcal S$ of the environment is continuous and given by
$$\mathcal S = \mathbb R^{33}$$, whereas the action space $$\mathcal A = \left\{
0,1,2,3 \right\}$$ is discrete and independent of the current environment state
(i.e.\ all four actions are available irrespective of the current state). The state-action value
function $$Q_{\theta}: \mathcal S \times \mathcal A \mapsto \mathbb R$$ maps a
state-action pair to an estimated expected discounted cumulative reward
generated by an agent following a greedy policy $\pi_{\theta}$ derived from $$Q_{\theta}$$.
The parameter vector $\theta$ determining the state-action value function
$$Q_{\theta}$$ takes values in a finite-dimensional vector space $$\mathbb R^{p}$$. 

# Training analysis

Despite its simplicity, the agent described in this report is able to solve the
environment after completing less than $1000$ episodes.

# Ideas for future work





