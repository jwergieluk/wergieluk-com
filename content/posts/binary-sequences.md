---
title: "Serial dependence in random binary sequences"
date: 2020-04-05
draft: true
katex: true
markup: "mmark"
modified: 2020-04-06
---

In this blog post I am going to investigate serial dependence in random binary
sequences.

* Binary sequences are sequences of of zeros and ones. 

It's about binary sequences that flag occurrence of some random events. In this
context, we observe a system at discrete time instances and produce zeros if
not event occurred and one otherwise.  

TODO: Describe a simple example and work with in through the article.

## Remarks to be mixed into the text later

* I am interested in the statistical formulation of the problem. Given a finite
  sequence of zeros and ones, collect a set of methods to gather arguments to
  support or reject the hypothesis that this sequence was generated by sampling
  from a Bernoulli random variable with success probability $$p$$.
* Is a given sequence a random noise or is there some underlying pattern. 
* In particular, I am interested in testing whether there is a dependence
  between the events. Do these events tend to occur in clusters? 
* List required knowledge
* What are we going to learn here?

## Probabilistic formulation of the problem.

Given a sequence of random variables $$X = (X_0, X_1, ..., X_n)$$ taking values
in the set $${0, 1}$$ and a probability $$p\in(0,1)$$, I would like to investigate
the serial dependencies between the elements of $$X$$. This investigation should be
based on a sample $$x$$ drawn from $$X$$, i.e. a finite sequence of zeros and ones 
$$x_0, x_1,.., x_n$$. This is a very difficult and deep problem, and in this
blog post I am going to focus collecting evidence to support or reject the following
two basic hypotheses:

1. The random variables $$X_i$$ have all the same distribution $$\text{Ber}(p)$$.
2. The random variables $$X_i$$ are independent. 

If we assume that the elements of $$x$$ are all samples from a fixed Bernoulli random
variable $$X_0$$, then we can estimate the probability $$p$$ by calculating the
average of $$x_0, x_1, ..., x_n$$. This is because the expectation of $$X_0$$
is $$\mathbb E X_0 = p$$.

Random binary sequences $$x$$ are often associated with discreet observations
of a system with two states. We say that "an event" occurred at time $$i$$, if
$$x_i = 1$$.  Also, for a Bernoulli random variable $$X_0 \sim\text{Ber}(p)$$,
$$p = \mathbb P (X_0 = 1)$$ is called the success probability.

## Serial dependence

How can we check whether our events occur independently of each other and 
gather evidence that there is not serial dependence? 

Examples of dependent events is when occurrence of an event some how impacts
the occurrence probabilities of subsequent events. Typical example is when
something breaks and gets repaired. 

Lucky and unlucky streaks. Run of luck. Winning streak. 

Mention the problem of run probabilities and the our intuitive understanding of
independence is often not correct.

In order to investigate the question of serial dependence in the sequence $$X$$
based on the observation $$x$$, we can look at the distribution of waiting
times between the events (lags). For example the sequence 

    [0, 1, 1, 0, 0, 1, 0, 0],

yields the following sequence of waiting times

    [1, 3].

Note that the waiting time calculation discards the initial and trailing zeros
of the event sequence $$x$$. For an i.i.d. sequence of Bernoulli random
variables, the sequence of waiting times consists of i.i.d. random variables
with geometric distribution. 

## The Geometric Distribution

The probability mass function of the geometric distribution with parameter $$p$$ is given by 

$$w(k) = \mathbb P(Z = k) = (1-p)^{k-1} p,\ k=1,2,...$$

For $$p=0.1$$, the PMF of the geometric distribution looks as follows.

{{< figure src="/binary_sequences/geom_pmf.png" >}}

One way of test whether a waiting times sequence follows geometric
distribution is to look at the orthogonal polynomials generated by
that distribution.

Orthogonal polynomials are an old technology.

For a given probability distribution $$\mu$$ on $$\mathbb R$$ we can 
define a scalar product between square integrable functions $$f, g \in L_2(\mu)$$
as 

$$\langle f,g \rangle = \int f(x) g(x) \mu(dx) = \mathbb E f(Y)g(Y),$$

where $$Y$$ is a random variable with distribution $$\mu$$.
Also a scalar product defines the notion of orthogonality between the functions
in $$L_2(\mu)$$: $$f$$ and $$g$$ are orthogonal iff $$\langle f,g \rangle = 0$$.
Finally, as sequence of polynomials $$(p_i(x))_{i\geq 0}$$ is called orthogonal 
if $$\langle p_k, p_i \rangle = 0$$ for all $$k \neq i$$. 

In this light, there is an intimate connection between probability distributions
and families of orthogonal polynomials.

For a geometric probability distribution characterized by the above pmf, where
is a corresponding family of orthogonal polynomials $$M = (M_i(x))_{i\geq 0}$$.
The family $$M$$ is a special case of the Meixner family of orthogonal
polynomials that is derived from negative binomail distribution (a
generalization of the geometric distribution). The members of the Meixner
family satisfy the following handy recursive relation:

$$ M_{j+1}(x) = \frac{ (1-p)(2j+1) + p(j-x+1)}{(j+1)\sqrt{1-p}} M_j(x) - \frac{j}{j+1} M_{j-1}(x), $$

with $$M_1(x) = 1$$ and $$M_{-1}(x) = 0$$. This relation is used to calculate 
the sequence $$M$$.

In the companion python source code [^1], the function `meixner_poly_eval` is used
to evaluate Meixner polynomials up to a given degree on a given set of points.
I used this function to plot the graphs of these polynomials up to degree 50.

{{< figure src="/binary_sequences/meixner_polynomials.png" >}}

As mentioned above, every polynomial $$M_k(x)$$ in $$M$$ satisfies the equation

$$\mathbb E M_k(Y) = 0$$ 

for a geometric random variable $$Y$$. This relation can
be used to test whether a given sample of waiting times belongs to a geometric
distribution.  We are going to estimate the expectation and use that
as a score: Values close to zero can be interpreted as evidence speaking for
geometric distribution. If the values are far from zero, this is a sign that
we need to revisit our assumptions and possibly discard the i.i.d. hypothesis
about the original event flag sequence. Note that significant deviations from 
zero of the above expectations can also occurr in case the events are independend
but the true event probabilty $$p$$ differs significantly from our assumptions.
This can easily be tested as described above.

### Meixner dependency score

In order to arrive at a single number (a score) that quantifies the degree of serial dependence 
in the sense developed above, we can define the *Meixner dependency score (MDS)* as a mean
of expectation estimates for the first $$k$$ Meixner polynomials evalualted at a sample of 
waiting times $$x = (x_1,..,x_m)$$: 

$$D^k_{\text{meixner}}(x) = \frac{1}{k} \sum_{i=1}^{k} |\bar{M}_i(x)|.$$ 

The MC experiment with the i.i.d. Bernoulli model yields an MDS of $$0.08$$, whereas 
the our second model with a simple dependence structure has an MDS of $$0.17$$.

Remark that the histograms often overlap, which makes the results unconclusive. 

## Monte-Carlo study

To test whether the procedure divesed above has a change to work in practive, 
I am going to test it using synthetically generated data. This type of testing
procedure is commonly known as Monte-Carlo (MC) study. Obviously it doesn't 
replace tests with real-world data, but can help evaluate a procedure under
controlled conditions. 

I am going to perform the following experiment:

1. Generate a binary sequence using a model with known serial dependence structure
2. Estimate the expectations $$\mathbb E M_k(X)$$ for $$k=1,...,10$$.
3. Repeat the points 1 and 2 over a large number of independent trials and
   visualize the aggregated results. 

Let's start with a simple case of simulating an i.i.d. sequence of Bernoulli
random variables with success probability $$p_0$$. This can be used 
as a sanity check and to test whether our implementation of the procedure is correct.

For the following experiment we set $$p_0 = 0.05$$ and simulate 5000 samples from
$$X = (X_1,.., X_1000). 

{{< figure src="/binary_sequences/j_cc_hist_0.05.png" >}}

Let's introduce a model with a simple form of serial dependence. 
For that we select probabilities $$0 < p_0 < p_1 < 1$$ and set the distributions
of random variables in the sequence $$X = (X_0, X_1, X_2, ...)$$ as follows.
Let $$X_0$$ be Bernoulli with success probablity $$p_0$$. The distributions
of $$X_i$$ for $$i>0$$ are conditional on $$X_{i-1}$$:

$$
\begin{aligned}
\mathbb P(X_i = 1 | X_{i-1} = 0) &= p_1, \\ 
\mathbb P(X_i = 1 | X_{i-1} = 1) &= p_2. 
\end{aligned}
$$

In order to compare this model with an i.i.d. Bernoulli model with the success
probability $$p_0$$, we need to set $$p_1$$ and $$p_2$$ such that the unconditional 
distributions are $$X_i\sim\text{Ber}(p_0)$$. Obviously, we will have
$$p_1 < p_0 < p_2$$.

We dedicate a special section at the end of this blog post to the derivation of the
probabilities $$p_1$$ and $$p_2$$ given $$p_0$$.

Let's test the dependency scoring algorithm for the following values $$p_1$$ and $$p_2$$.
An MC evaluation of the Meixner polynomials leads to the following Meixner dependency scores: 

| p1  | p2   | MDS |
| --- | ---- | --- |
| 0.5 | 0.5  | 0.083 |
| 0.4 | 0.24 | 0.194 |
| 0.3 | 0.43 | 0.382 |
| 0.2 | 0.62 | 0.583 |

Again, 5000 simulations of sample with length 1000 yield the following histograms for 
the expectations. 

{{< figure src="/binary_sequences/j_cc_hist_0.02.png" >}}

As we can see the histograms are shifted slightly to the left. 

### Closing remarks

The method cannot pick up very subtle dependence patterns, especially if the sample size is small. 

What have we learned from this?

### Derivation of $$p_1$$ and $$p_2$$ using Markov chains

For a given $$0 < p_1 < p_0 < 1$$, we are looking for $$p_2\in (0, 1)$$, such
that the random binary sequence defined above satisfies $$P(X_i = 1) = p_1$$.
This problem doesn't have a solution is we require that $$P(X_i = 1) = p_1$$
holds for all $$i>0$$. Instead, let's require that this equation is satisfied
approximately for all moderately large $$i$$, say $$i>20$$. (This requirement
could be replaced with a exact requirement on the asymptotic distribution, i.e.
$$\lim_{i\to\infty} P(X_i = 1) = p_1$$.)

State space with four states: $$(0,0), (0,1), (1,0), (1,1)$$.

Transition probability matrix:
$$
\left(\begin{matrix}
  1-p_1 & p_1 & 0 & 0 \\
  0 & 0 & 1-p_2 & p_2 \\
  1-p_1 & p_1 & 0 & 0 \\
  0 & 0 & 1-p_2 & p_2 
\end{matrix}\right)
$$

Initial distribution is $$\lambda = (1-p1, p1, 0, 0)$$.

An engineers solution: Given $$p_0$$ and $$p_1$$ we calculate $$p_2$$ using an
optimization algorithm.

## Examples and use cases

* VaR violations. 
* Random number generators usually produce binary sequences that are
  transformed deterministically into some "more interesting" random numbers.
  There are test suites that test how random is a sequence produced by a rng.
  Example: Die hard test suite.
* Kaggle competitions or data sets?

## Ideas

* Train a Markov process and look at it's transition probability matrix.

## References

* Bertrand Candelon, Gilbert Colletaz, Christophe Hurlin, Sessi Tokpavi,
  Backtesting Value-at-Risk: A GMM Duration-Based Test, Journal of Financial
  Econometrics, Volume 9, Issue 2, Spring 2011, Pages 314–343,
  http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.404.3188&rep=rep1&type=pdf
* Christoffersen, Peter and Pelletier, Denis, Backtesting Value-at-Risk: A
  Duration-Based Approach (January 31, 2003). Available at SSRN:
  https://ssrn.com/abstract=418762 or http://dx.doi.org/10.2139/ssrn.418762 
* Olver, Frank WJ, Daniel W. Lozier, Ronald F. Boisvert, and Charles W. Clark,
  eds. NIST handbook of mathematical functions hardback and CD-ROM. Cambridge
  university press, 2010.


[^1]: meixner.py

<!-- vim: set syntax=markdown: set spelllang=en: set spell: -->
