---
title: "Risk stabilized portfolios (draft)"
date: 2020-12-29
draft: true
katex: true
markup: "mmark"
tags:
    - machine-learning
    - probability-theory
---

Here is a rough plan for this project.
* Get a bunch of asset price time-series.
* Write down few discrete-time martingale models for the log-returns of these time series. Add constant dependence
  structure models to get multi-series models.
* Use the models to simulate and forecast one-day ahead price distributions.
* Construct a set of random constant-mix portfolios.
* Forecast and measure the predictive quality delivered by the models. Use these quality measures to come up with a score.
* Select good models.
* Construct risk-budgeting portfolios.

Let $$\left( \Omega, \mathcal A, \mathcal F, P \right)$$ be a stochastic basis
with a complete $$\sigma$$-algebra $$\mathcal A$$ of measurable subsets of
$$\Omega$$, a probability measure $$P$$, and a filtration $$\mathcal{F} =
(\mathcal{F}_t)_{t\in\mathbb{Z}}$$.  I consider discrete-time real-valued
processes $$X = (X_t)_{t\in\mathbb{Z}}$$, with $$X_s \in \mathbb{R}^d$$. 

The process $$X$$ models the evolution of log-returns of $$d$$ asset prices
observed daily. I construct $$X$$ by following the usual two-step program:
1. Specify the dynamics of the one-dimensional components of $$X$$, separately.
2. Specify the dependence model for the noise process $$Z$$.

All models considered here have the dynamics of the form

$$ X_t = \sigma_t Z_t, $$

where $$Z$$ is a white noise process, and the "volatility" process $$\sigma$$
is predictable (i.e. $$\sigma_t \in \mathcal{F}_{t-1}$$).

# One-dimensional models

## M0

## M4

In the defining equation $$X_t = \sigma_t Z_t,$$ I assume that the process
$$Z$$ is an i.i.d. Student-t noise with the degrees of freedom parameter
$$\delta$$, and the volatility process $$\sigma$$ satisfies

$$\sigma^2_t = \gamma_0 \sum_{s < t} \nu_\kappa(X)_s \omega_{s-t}^0 + \gamma_1 \sum_{s < t} \sigma^2_s \omega^1_{s-t}.$$

The dynamics of $$\sigma$$ uses a "skewed parabola" function to account for the
"leverage effect" known from the financial literature:

$$
\begin{aligned}
\nu_\kappa(x) &= 4 \left( (1-\kappa) x^- + \kappa x^+ \right)^2,\\
x^{-} &= \min (0, x), \\
x^+ &= \max (0, x). 
\end{aligned}
$$

{{< figure src="/garch/skewed_parabola.png" >}}

Note that $$\nu_{\kappa}(x) = x^2$$ for $$\kappa = \frac{1}{2}$$.

The exponential decay weight sequences $$\omega^0$$ and $$\omega^1$$ are
parametrized by the positive decay parameters $$\lambda_0$$ and $$\lambda_1$$.

$$\tilde\omega^j_i = \exp( \lambda_j i ), \ i=-1, -2, -3, \ldots, -N,$$

$$\omega^j = \tilde\omega^j / \sum_i \tilde\omega^j_i.$$

The sequence $$\tilde\omega^j$$ vanishes for all the values of $$i$$ other than
those used in the above definition.

Also, for simplicity, I assume that $$\delta = 6$$, and thus the model 
is parametrized by the $$5$$-dimensional vector

$$\theta = (\gamma_0, \gamma_1, \lambda_0, \lambda_1, \kappa).$$

## Noise dependence models

So far I only have constant dependence models. Early experiments confirmed that the multivariate t noise yields
better results to the standard normal noise. 

### Multivariate t noise

Density of a (non-degenerate) multivariate normal distribution with zero mean is given by
\\[
f_X(x_1, \ldots, x_n) = \frac{1}{\sqrt{(2\pi)^n |\Sigma|}} 
\exp \Big( -\frac{1}{2} x^\top \Sigma^{-1} x \Big),
\\]
where \\(\Sigma\\) is a (positive-semidefinite) covariance matrix.

For comparison, the density of the multivariate t-distribution with zero mean is
\\[
\frac{\Gamma[(\nu + n)/2]}{\Gamma(\nu/2) \sqrt{v^{n} \pi^{n} |\Sigma|}}
\Big[ 1 + \frac{1}{\nu} x^\top \Sigma^{-1} x  \Big]^{-(\nu + n)/2}.
\\]

Compare this with the univariate version of the t density:
\\[
\frac{\Gamma{(\nu +1)/2}}{\Gamma(\nu/2)\sqrt{\nu\pi}} \Big[ 1 + \frac{x^2}{2} \Big]^{-(\nu+1)/2}
\\]

Gregory Grundsen
[provides](https://gregorygundersen.com/blog/2020/01/20/multivariate-t/) a
Python code to evaluate the multivariate t-density based on SciPy library.

A standard way to construct a random variable \\(Z\\) with t-distribution is to use
a normal random variable \\(X\\) and an independent chi-square random variable \\(Y\\) and set
\\[Z = \frac{X}{\sqrt{Y/n}}. \\]
This formula works for both the univariate and the multivariate case and can be
used to draw samples from the t distribution using samples from \\(X\\) and
\\(Y\\).

$$\Sigma$$ is not the covariance matrix of $$Z$$, but, for $$\nu>2$$, the covariance matrix of $$Z$$ is 

$$
\frac{\nu}{\nu -2}\Sigma.
$$

If $$\nu$$ is assumed to be known in advance, the parameter estimation for $$Z$$ boils down
to the calculation of the covariance matrix of the observations. This is probably not a robust method.

# Parameter estimation

Maximum-likelihood (ML) parameter estimation is the method of choice for all
the discussed models since the transition density of $$X$$ is known explicitly.
Log-likelihood function of the observation path $$x$$ is thus given by

$$
\ln L(\theta; x) = \sum_{s=1}^t \ln \eta_Z \left( \frac{x_t - \mu_t(\theta)}{\sigma_t(\theta)} \right) - \ln \sigma_t(\theta),
$$

where $$\eta_Z$$ is the density of $$Z$$.

Every day, a new observation is appended to $$x$$ and the model parameters are
recalibrated. A common practice in finance is to replace the above
log-likelihood function with a weighted version that emphasises the more recent
information. Also, usually the risk-management models are calibrated using a
fixed history length (e.g. 4 years). Thus a rolling window of historical
observations is used. 

On the other hand, we have the "machine learning" style parameter optimization
methods, that seek to identify a training set which is used to find good model
parameters. These parameters are fixed during the training phase and held
constant "in production" unless a significant model performance degradation is
observed.

# Model selection

A: What is a good model?

B: One that is good at forecasting portfolio risk measured by CVaR.

A: But it's not obvious how to test whenever a model can forecast CVaR well.

B: That's true, but if a model can forecast the future return distributions well, then it will be able to forecast 
CVaR too.

A: How do you want to test, whether a model can forecast the future return distribution well? Using models
likelihood function seems not a very good idea, because likelihood depends on the model. And, I would like to 
compare different models, so the measure of model quality should be independent of the model construction. 

So can I measure the quality of probability measure forecast in a model-independent way?



* What are your selection criteria?
* What is your baseline model?

We have some hyperparameters! (Estimator decay factors)

https://www.imo.universite-paris-saclay.fr/~arlot/papers/14hdr.pdf


# References

* https://en.wikipedia.org/wiki/Multivariate_t-distribution
* Michael Roth. On the Multivariate t Distribution. https://www.diva-portal.org/smash/get/diva2:618567/FULLTEXT02
* Gregory Grundsen. A Python Implementation of the Multivariate t-distribution. 
  https://gregorygundersen.com/blog/2020/01/20/multivariate-t/
* KaTeX cheat-sheet: https://katex.org/docs/supported.html


<!-- vim: set syntax=markdown: set spelllang=en_us: set spell: -->

